
```{python}
import pandas as pd
import altair as alt
import time
```

Pseudo-Code
```{python}
def scrape_enforcement_actions(year, month):
    """
    Pseudo-code for scraping enforcement actions from a given month/year to present
    
    Parameters:
    year (int): Starting year to scrape from
    month (int): Starting month to scrape from
    """
```
FUNCTION scrape_enforcement_actions(year, month):

1. INPUT VALIDATION
   IF year is less than 2013
      Print "Please enter a year >= 2013"
      Exit function
   
2. SETUP
   Create empty lists for storing:
   - titles
   - dates
   - categories
   - links
   - agencies
   Set base URL to HHS OIG enforcement page

3. PAGINATION LOOP (WHILE loop)
   WHILE there are more pages to process:
      Get current page content
      FOR each enforcement action on page:
         Extract title and link
         Get agency from detailed page
         Get date and category
         Store all information in lists
         Wait 1 second before next action
      
      Look for "next page" link
      IF found:
         Update URL to next page
         Wait 1 second before next page
      ELSE:
         Exit WHILE loop

4. DATA PROCESSING
   Create DataFrame from collected lists
   Convert dates to proper format
   Filter to keep only entries >= input month/year

5. SAVE RESULTS
   Create filename: "enforcement_actions_[year]_[month].csv"
   Save DataFrame to CSV file

6. OUTPUT
   Return the filtered DataFrame

Dynamic Scraper:

I will use ThreadPoolExecutor to get data much more faster than the regular process. And I used several strategies to avoid frequent timeout and connection errors and consistent failures on specific pages (like pages 192 and 193).

```{python}
import pandas as pd
import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import random
import time
import sys

# add restriction
sys.setrecursionlimit(10000)

def fetch_detailed_info(link, session=None, max_retries=3):
    if session is None:
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    for attempt in range(max_retries):
        try:
            response = session.get(link, timeout=10)
            if response.status_code == 200:
                detailed_soup = BeautifulSoup(response.content, "html.parser")
                agency = 'No agency found'
                detail_div = detailed_soup.find('div', class_='margin-top-5 padding-y-3 border-y-1px border-base-lighter')
                if detail_div:
                    all_li = detail_div.find_all('li')
                    for li in all_li:
                        span = li.find('span')
                        if span and 'Agency:' in span.text:
                            agency = li.text.replace('Agency:', '').strip()
                            break
                return agency
            
            if attempt < max_retries - 1:
                time.sleep(1)
                
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(1)
            else:
                print(f"Error processing {link}: {str(e)}")
    return 'Error retrieving agency'

def fetch_page_data(page_num, session=None, max_retries=3):
    if session is None:
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    base_url = "https://oig.hhs.gov/fraud/enforcement/"
    for attempt in range(max_retries):
        try:
            current_url = f"{base_url}?page={page_num}" if page_num > 1 else base_url
            response = session.get(current_url, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                entries = soup.find_all('h2', class_='usa-card__heading')
                
                page_data = []
                for entry in entries:
                    link = entry.find('a')
                    if link:
                        title = link.text.strip()
                        full_link = 'https://oig.hhs.gov' + link['href']
                        div = entry.find_parent('div')
                        if div:
                            date = div.find('span', class_='text-base-dark padding-right-105')
                            date_text = date.text.strip() if date else 'No date found'
                            category_tags = div.find_all('li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1')
                            entry_categories = [cat.text.strip() for cat in category_tags] if category_tags else ['NA']
                            page_data.append({
                                'Title': title,
                                'Date': date_text,
                                'Categories': entry_categories,
                                'Link': full_link
                            })
                return page_data if page_data else None
            
            if attempt < max_retries - 1:
                time.sleep(1)
                
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(1)
            else:
                print(f"Error on page {page_num}: {str(e)}")
    return None

def scrape_enforcement_actions(year, month, max_pages=482):
    if year < 2013:
        print("Please enter a year >= 2013")
        return None
    
    data_list = []
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    })
    
    # Stage One: quickly fetch and read the page
    print("Fetching pages...")
    failed_pages = set()
    
    with ThreadPoolExecutor(max_workers=8) as executor:
        future_to_page = {
            executor.submit(fetch_page_data, page, session): page 
            for page in range(1, max_pages + 1)
        }
        
        for future in as_completed(future_to_page):
            page = future_to_page[future]
            try:
                page_data = future.result()
                if page_data:
                    data_list.extend(page_data)
                    print(f"Added data from page {page}")
                else:
                    failed_pages.add(page)
            except Exception as e:
                failed_pages.add(page)
                print(f"Error on page {page}: {str(e)}")
    
    # Stage Two: Dealing with failed pages only
    if failed_pages:
        print(f"\nRetrying {len(failed_pages)} failed pages...")
        for page in sorted(failed_pages):
            try:
                page_data = fetch_page_data(page, session, max_retries=5)
                if page_data:
                    data_list.extend(page_data)
                    failed_pages.remove(page)
                    print(f"Successfully retrieved page {page} in retry")
            except Exception as e:
                print(f"Failed to retrieve page {page} in retry: {str(e)}")
    
    print(f"\nCollected {len(data_list)} total entries")
    
    # Stage three: quickly fetching agency information
    print("Fetching agency information...")
    with ThreadPoolExecutor(max_workers=10) as executor:
        future_to_entry = {
            executor.submit(fetch_detailed_info, entry['Link'], session): i 
            for i, entry in enumerate(data_list)
        }
        
        completed = 0
        total = len(data_list)
        
        for future in as_completed(future_to_entry):
            try:
                idx = future_to_entry[future]
                agency = future.result()
                data_list[idx]['Agency'] = agency
                completed += 1
                if completed % 20 == 0:
                    print(f"Processed {completed}/{total} entries")
            except Exception as e:
                print(f"Error processing agency info: {str(e)}")

    # Create separate rows version
    rows_separate = []
    for entry in data_list:
        for category in entry['Categories']:
            rows_separate.append({
                'Title': entry['Title'],
                'Date': entry['Date'],
                'Category': category,
                'Link': entry['Link'],
                'Agency': entry.get('Agency', 'Error retrieving agency')
            })

    # Create combined categories version
    rows_combined = []
    for entry in data_list:
        rows_combined.append({
            'Title': entry['Title'],
            'Date': entry['Date'],
            'Category': ' and '.join(entry['Categories']),
            'Link': entry['Link'],
            'Agency': entry.get('Agency', 'Error retrieving agency')
        })

    # Create DataFrames
    df_separate = pd.DataFrame(rows_separate)
    df_combined = pd.DataFrame(rows_combined)
    
    # Convert date and filter
    df_separate['Date'] = pd.to_datetime(df_separate['Date'], errors='coerce')
    df_separate = df_separate[df_separate['Date'] >= f"{year}-{month:02d}-01"]
    
    df_combined['Date'] = pd.to_datetime(df_combined['Date'], errors='coerce')
    df_combined = df_combined[df_combined['Date'] >= f"{year}-{month:02d}-01"]
    
    # Save to CSV
    filename_separate = f"enforcement_actions_{year}_{month}_separate.csv"
    filename_combined = f"enforcement_actions_{year}_{month}_combined.csv"
    
    df_separate.to_csv(filename_separate, index=False)
    df_combined.to_csv(filename_combined, index=False)
    
    print(f"\nSaved separate categories version to: {filename_separate}")
    print(f"Saved combined categories version to: {filename_combined}")
    
    return df_combined
```

```{python}
#| eval: false

#scrape data from year 2023 January
if __name__ == "__main__":
    try:
        df = scrape_enforcement_actions(2023, 1)
        if df is not None:
            print(f"\nTotal number of enforcement actions: {len(df)}")
            print("\nEarliest enforcement action:")
            earliest = df.sort_values('Date').iloc[0]
            print(f"Date: {earliest['Date']}")
            print(f"Title: {earliest['Title']}")
            print(f"Agency: {earliest['Agency']}")
            print(f"Categories: {earliest['Category']}")
    except Exception as e:
        print(f"Error running scraper: {str(e)}")
```

Total number of enforcement actions: 1534
Earliest enforcement action:
Date: 2023-01-03 00:00:00
Title: Podiatrist Pays $90,000 To Settle False Billing Allegation
Agency: U.S. Attorney’s Office, Southern District of Texas
Categories: Criminal and Civil Actions


```{python}
#| eval: false
# Run scraper from January 2021
if __name__ == "__main__":
    try:
        df = scrape_enforcement_actions(2021, 1)
        if df is not None:
            print(f"\nTotal number of enforcement actions: {len(df)}")
            print("\nEarliest enforcement action:")
            earliest = df.sort_values('Date').iloc[0]
            print(f"Date: {earliest['Date']}")
            print(f"Title: {earliest['Title']}")
            print(f"Agency: {earliest['Agency']}")
            print(f"Categories: {earliest['Category']}")
    except Exception as e:
        print(f"Error running scraper: {str(e)}")
```

Total number of enforcement actions:1534
Earliest enforcement action:
Date: 2021-01-04 00:00:00
Title: The United States And Tennessee Resolve Claims With Three Providers For False Claims Act Liability Relating To ‘P-Stim’ Devices For A Total Of $1.72 Million Agency: U.S. Attorney's Office, Middle District of Tennessee
Categories: Criminal and Civil Actions

